# Update the dataframe with the modified "cyr" column
modified_df_new$cyr <- cyr_values
# Lets do analysis for cyr
cat("Unique values in cyr column:", unique(modified_df_new$cyr), "\n")
cat("Minimum value in cyr column:", min(modified_df_new$cyr, na.rm = TRUE), "\n")
cat("Maximum value in cyr column:", max(modified_df_new$cyr, na.rm = TRUE), "\n")
cat("Number of NA values in cyr column:", sum(is.na(modified_df_new$cyr)), "\n")
# Lets do analysis for ladprox
cat("Unique values in ladprox column:", unique(modified_df_new$ladprox), "\n")
cat("Minimum value in ladprox column:", min(modified_df_new$ladprox, na.rm = TRUE), "\n")
cat("Maximum value in ladprox column:", max(modified_df_new$ladprox, na.rm = TRUE), "\n")
cat("Number of NA values in ladprox column:", sum(is.na(modified_df_new$ladprox)), "\n")
# Assuming modified_df_new is your dataframe
ladprox_values <- modified_df_new$ladprox
# Calculate the mode of the "ladprox" column
mode_ladprox <- as.character(names(sort(table(ladprox_values), decreasing = TRUE)[1]))
# Replace NA values with the mode
ladprox_values[is.na(ladprox_values)] <- mode_ladprox
# Update the dataframe with the modified "ladprox" column
modified_df_new$ladprox <- ladprox_values
# Lets do analysis for ladprox
cat("Unique values in ladprox column:", unique(modified_df_new$ladprox), "\n")
cat("Minimum value in ladprox column:", min(modified_df_new$ladprox, na.rm = TRUE), "\n")
cat("Maximum value in ladprox column:", max(modified_df_new$ladprox, na.rm = TRUE), "\n")
cat("Number of NA values in ladprox column:", sum(is.na(modified_df_new$ladprox)), "\n")
# Lets do analysis for cxmain
cat("Unique values in cxmain column:", unique(modified_df_new$cxmain), "\n")
cat("Minimum value in cxmain column:", min(modified_df_new$cxmain, na.rm = TRUE), "\n")
cat("Maximum value in cxmain column:", max(modified_df_new$cxmain, na.rm = TRUE), "\n")
cat("Number of NA values in cxmain column:", sum(is.na(modified_df_new$cxmain)), "\n")
# Assuming modified_df_new is your dataframe
cxmain_values <- modified_df_new$cxmain
# Calculate the mode of the "cxmain" column
mode_cxmain <- as.character(names(sort(table(cxmain_values), decreasing = TRUE)[1]))
# Replace NA values with the mode
cxmain_values[is.na(cxmain_values)] <- mode_cxmain
# Update the dataframe with the modified "cxmain" column
modified_df_new$cxmain <- cxmain_values
# Lets do analysis for cxmain
cat("Unique values in cxmain column:", unique(modified_df_new$cxmain), "\n")
cat("Minimum value in cxmain column:", min(modified_df_new$cxmain, na.rm = TRUE), "\n")
cat("Maximum value in cxmain column:", max(modified_df_new$cxmain, na.rm = TRUE), "\n")
cat("Number of NA values in cxmain column:", sum(is.na(modified_df_new$cxmain)), "\n")
# Lets do analysis for lvx1
cat("Unique values in lvx1 column:", unique(modified_df_new$lvx1), "\n")
cat("Minimum value in lvx1 column:", min(modified_df_new$lvx1, na.rm = TRUE), "\n")
cat("Maximum value in lvx1 column:", max(modified_df_new$lvx1, na.rm = TRUE), "\n")
cat("Number of NA values in lvx1 column:", sum(is.na(modified_df_new$lvx1)), "\n")
# Assuming modified_df_new is your dataframe
lvx1_values <- modified_df_new$lvx1
# Calculate the mode of the "lvx1" column
mode_lvx1 <- as.character(names(sort(table(lvx1_values), decreasing = TRUE)[1]))
# Replace NA values with the mode
lvx1_values[is.na(lvx1_values)] <- mode_lvx1
# Update the dataframe with the modified "lvx1" column
modified_df_new$lvx1 <- lvx1_values
# Lets do analysis for lvx1
cat("Unique values in lvx1 column:", unique(modified_df_new$lvx1), "\n")
cat("Minimum value in lvx1 column:", min(modified_df_new$lvx1, na.rm = TRUE), "\n")
cat("Maximum value in lvx1 column:", max(modified_df_new$lvx1, na.rm = TRUE), "\n")
cat("Number of NA values in lvx1 column:", sum(is.na(modified_df_new$lvx1)), "\n")
# Lets do analysis for lvx2
cat("Unique values in lvx2 column:", unique(modified_df_new$lvx2), "\n")
cat("Minimum value in lvx2 column:", min(modified_df_new$lvx2, na.rm = TRUE), "\n")
cat("Maximum value in lvx2 column:", max(modified_df_new$lvx2, na.rm = TRUE), "\n")
cat("Number of NA values in lvx2 column:", sum(is.na(modified_df_new$lvx2)), "\n")
# Assuming modified_df_new is your dataframe
lvx2_values <- modified_df_new$lvx2
# Calculate the mode of the "lvx2" column
mode_lvx2 <- as.character(names(sort(table(lvx2_values), decreasing = TRUE)[1]))
# Replace NA values with the mode
lvx2_values[is.na(lvx2_values)] <- mode_lvx2
# Update the dataframe with the modified "lvx2" column
modified_df_new$lvx2 <- lvx2_values
# Lets do analysis for lvx2
cat("Unique values in lvx2 column:", unique(modified_df_new$lvx2), "\n")
cat("Minimum value in lvx2 column:", min(modified_df_new$lvx2, na.rm = TRUE), "\n")
cat("Maximum value in lvx2 column:", max(modified_df_new$lvx2, na.rm = TRUE), "\n")
cat("Number of NA values in lvx2 column:", sum(is.na(modified_df_new$lvx2)), "\n")
# Lets do analysis for lvx3
cat("Unique values in lvx3 column:", unique(modified_df_new$lvx3), "\n")
cat("Minimum value in lvx3 column:", min(modified_df_new$lvx3, na.rm = TRUE), "\n")
cat("Maximum value in lvx3 column:", max(modified_df_new$lvx3, na.rm = TRUE), "\n")
cat("Number of NA values in lvx3 column:", sum(is.na(modified_df_new$lvx3)), "\n")
# Assuming modified_df_new is your dataframe
lvx3_values <- modified_df_new$lvx3
# Calculate the mode of the "lvx3" column
mode_lvx3 <- as.character(names(sort(table(lvx3_values), decreasing = TRUE)[1]))
# Replace NA values with the mode
lvx3_values[is.na(lvx3_values)] <- mode_lvx3
# Update the dataframe with the modified "lvx3" column
modified_df_new$lvx3 <- lvx3_values
# Lets do analysis for lvx3
cat("Unique values in lvx3 column:", unique(modified_df_new$lvx3), "\n")
cat("Minimum value in lvx3 column:", min(modified_df_new$lvx3, na.rm = TRUE), "\n")
cat("Maximum value in lvx3 column:", max(modified_df_new$lvx3, na.rm = TRUE), "\n")
cat("Number of NA values in lvx3 column:", sum(is.na(modified_df_new$lvx3)), "\n")
# Lets do analysis for lvx4
cat("Unique values in lvx4 column:", unique(modified_df_new$lvx4), "\n")
cat("Minimum value in lvx4 column:", min(modified_df_new$lvx4, na.rm = TRUE), "\n")
cat("Maximum value in lvx4 column:", max(modified_df_new$lvx4, na.rm = TRUE), "\n")
cat("Number of NA values in lvx4 column:", sum(is.na(modified_df_new$lvx4)), "\n")
# Assuming modified_df_new is your dataframe
lvx4_values <- modified_df_new$lvx4
# Calculate the mode of the "lvx4" column
mode_lvx4 <- as.character(names(sort(table(lvx4_values), decreasing = TRUE)[1]))
# Replace NA values with the mode
lvx4_values[is.na(lvx4_values)] <- mode_lvx4
# Update the dataframe with the modified "lvx4" column
modified_df_new$lvx4 <- lvx4_values
# Lets do analysis for lvx4
cat("Unique values in lvx4 column:", unique(modified_df_new$lvx4), "\n")
cat("Minimum value in lvx4 column:", min(modified_df_new$lvx4, na.rm = TRUE), "\n")
cat("Maximum value in lvx4 column:", max(modified_df_new$lvx4, na.rm = TRUE), "\n")
cat("Number of NA values in lvx4 column:", sum(is.na(modified_df_new$lvx4)), "\n")
# Lets do analysis for lvf
cat("Unique values in lvf column:", unique(modified_df_new$lvf), "\n")
cat("Minimum value in lvf column:", min(modified_df_new$lvf, na.rm = TRUE), "\n")
cat("Maximum value in lvf column:", max(modified_df_new$lvf, na.rm = TRUE), "\n")
cat("Number of NA values in lvf column:", sum(is.na(modified_df_new$lvf)), "\n")
# Assuming modified_df_new is your dataframe
lvf_values <- modified_df_new$lvf
# Calculate the mode of the "lvf" column
mode_lvf <- as.character(names(sort(table(lvf_values), decreasing = TRUE)[1]))
# Replace NA values with the mode
lvf_values[is.na(lvf_values)] <- mode_lvf
# Update the dataframe with the modified "lvf" column
modified_df_new$lvf <- lvf_values
# Lets do analysis for lvf
cat("Unique values in lvf column:", unique(modified_df_new$lvf), "\n")
cat("Minimum value in lvf column:", min(modified_df_new$lvf, na.rm = TRUE), "\n")
cat("Maximum value in lvf column:", max(modified_df_new$lvf, na.rm = TRUE), "\n")
cat("Number of NA values in lvf column:", sum(is.na(modified_df_new$lvf)), "\n")
# Lets do analysis for num
cat("Unique values in num column:", unique(modified_df_new$num), "\n")
cat("Minimum value in num column:", min(modified_df_new$num, na.rm = TRUE), "\n")
cat("Maximum value in num column:", max(modified_df_new$num, na.rm = TRUE), "\n")
cat("Number of NA values in num column:", sum(is.na(modified_df_new$num)), "\n")
# Assuming modified_df_new is your dataframe
num_na <- sum(is.na(modified_df_new))
# Display the number of NAs
cat("Number of NA values in the entire dataframe:", num_na, "\n")
###############################################################
# Assuming modified_df_new is your dataframe
numeric_df <- apply(modified_df_new, 2, function(x) as.numeric(as.character(x)))
modified_df_new <- numeric_df
m_df <- data.frame(numeric_df)
str(m_df)
###################################
l_df <- m_df
str(l_df)
# Assuming 'num' is a character column in latest_df
l_df$num <- as.numeric(as.character(l_df$num))
# Check if 'num' is now numeric
is_numeric <- is.numeric(l_df$num)
# Print the result
print(is_numeric)
# Assuming 'num' is a numeric or integer column in latest_df
l_df$num <- ifelse(l_df$num > 1, 1, l_df$num)
# Check the unique values in 'num' after conversion
unique(l_df$num)
View(l_df)
# Logistic Regression as an example
model <- glm(num ~ ., data = l_df, family = "binomial")
summary(model)
# Model Evaluation
predicted_probs <- predict(model, newdata = l_df, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# Assess accuracy
confusion_matrix <- table(l_df$num, predicted_classes)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)
# Calculate accuracy, sensitivity, and specificity
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
sensitivity <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
specificity <- confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
# Print evaluation metrics
cat("Accuracy:", accuracy, "\n")
print(sensitivity)
print(specificity)
##############################
aic <- AIC(model)
bic <- BIC(model)
cat("AIC:", aic, "\n")
cat("BIC:", bic, "\n")
# ROC-AUC
library(pROC)
roc_curve <- roc(l_df$num, predict(model, type = "response"))
roc_auc <- auc(roc_curve)
cat("ROC-AUC:", roc_auc, "\n")
###############################
#########################
# Assuming 'data' is your dataset with 'CAD' as the binary outcome variable
# and 'feature1', 'feature2', ... as predictor variables
# Split the data into training and testing sets
set.seed(123)  # Set seed for reproducibility
train_indices <- sample(1:nrow(latest_df), 0.8 * nrow(latest_df))
###############################
#########################
# Assuming 'data' is your dataset with 'CAD' as the binary outcome variable
# and 'feature1', 'feature2', ... as predictor variables
# Split the data into training and testing sets
set.seed(123)  # Set seed for reproducibility
train_indices <- sample(1:nrow(l_df), 0.8 * nrow(l_df))
train_data <- l_df[train_indices, ]
test_data <- l_df[-train_indices, ]
# Logistic Regression Model Training
model <- glm(num ~ feature1 + feature2 + ..., data = train_data, family = "binomial")
# Logistic Regression Model Training
model <- glm(num ~ ., data = train_data, family = "binomial")
# Logistic Regression Model Training
model <- glm(num ~ ., data = train_data, family = "binomial")
# Model Evaluation on Training Set
train_predicted_probs <- predict(model, newdata = train_data, type = "response")
train_predicted_classes <- ifelse(train_predicted_probs > 0.5, 1, 0)
# Training Error
train_confusion_matrix <- table(train_data$num, train_predicted_classes)
train_accuracy <- sum(diag(train_confusion_matrix)) / sum(train_confusion_matrix)
cat("Training Accuracy:", train_accuracy, "\n")
# Model Evaluation on Testing Set
test_predicted_probs <- predict(model, newdata = test_data, type = "response")
test_predicted_classes <- ifelse(test_predicted_probs > 0.5, 1, 0)
# Testing Error
test_confusion_matrix <- table(test_data$num, test_predicted_classes)
test_accuracy <- sum(diag(test_confusion_matrix)) / sum(test_confusion_matrix)
cat("Testing Accuracy:", test_accuracy, "\n")
####################
##################################
# Calculate the correlation matrix
cor_matrix <- cor(m_df, use = "complete.obs")
# Load the corrplot library
library(corrplot)
# Create a correlation plot
corrplot(cor_matrix, method = "circle")
#######################################333
# Set the correlation threshold
# Set the correlation threshold
threshold <- 0.6
# Find highly correlated pairs
highly_correlated_pairs <- which(upper.tri(cor_matrix, diag = FALSE) & abs(cor_matrix) > threshold, arr.ind = TRUE)
# Get the names of highly correlated columns
highly_correlated_features <- unique(c(rownames(cor_matrix)[highly_correlated_pairs[, 1]], rownames(cor_matrix)[highly_correlated_pairs[, 2]]))
# Print out the highly correlated columns
cat("Columns with correlation above", threshold, ":\n")
print(highly_correlated_pairs)
print(highly_correlated_features)
########################################################################3
########## removing  highly corellated features from each pair
column_names_todrop_cor <- c("dummy","cyr","rldv5","proto","met")
m_df <- m_df[, !colnames(m_df) %in% column_names_todrop_cor]
print(m_df)
View(m_df)
########################################################################3
# Extract features (excluding the target variable)
#features <- select(numeric_columns, -num)
# Standardize the features (important for PCA)
#scaled_features <- scale(features)
# Perform PCA
#pca_result <- prcomp(scaled_features)
# View summary of PCA
#summary(pca_result)
# Plot the cumulative proportion of variance explained
#plot(cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2)), xlab = "Number of Principal Components", ylab = "Cumulative Proportion of Variance Explained")
###########################################3
library(randomForest)
# Assuming your data is in 'data' and the response variable is 'response'
model <- randomForest(num ~ ., data = m_df)
# Get feature importance
feature_importance <- importance(model)
feature_importance
sorted_feature_importance <- feature_importance[order(-feature_importance[, 1]), ]
sorted_feature_importance
##################
#Since xhypo", "lvx2", "dig", "lvx1 are least significant, we are dropping them based on mannual anzlysis
column_names_todrop_rf <- c("xhypo", "lvx2", "dig", "lvx1")
m_df <- m_df[, !colnames(m_df) %in% column_names_todrop_rf]
View(m_df)
#################
##########-----####
# Assuming your data is in 'modified_df_new' and the response variable is 'num'
library(glmnet)
# Extracting predictors and response variable
response_variable <- "num"
predictors <- m_df[, names(m_df) != response_variable, drop = FALSE]
response <- m_df[, response_variable]
# Scale the predictors
scaled_predictors <- scale(predictors)
# Convert predictors to matrix
predictors_matrix <- as.matrix(scaled_predictors)
# Fit a Lasso (L1 regularization) model using cross-validation
lasso_model <- cv.glmnet(predictors_matrix, response, alpha = 1)
# Print the cross-validated results
print(lasso_model)
# Plot the cross-validated mean squared error as a function of log(lambda)
plot(lasso_model)
# Identify the optimal lambda (minimizing mean cross-validated error)
best_lambda <- lasso_model$lambda.min
cat("Best Lambda:", best_lambda, "\n")
# Get feature coefficients for the optimal lambda
lasso_coefficients <- coef(lasso_model, s = best_lambda)
print(lasso_coefficients)
# Assuming 'lasso_coefficients' is the coefficients obtained from the LASSO model
# and 'predictors' is the data frame of predictors
# Convert coefficients to data frame
lasso_coefficients_df <- data.frame(variable = rownames(lasso_coefficients)[-1], coefficient = as.numeric(lasso_coefficients)[-1])
# Sort coefficients by absolute value
sorted_lasso_coefficients <- lasso_coefficients_df[order(abs(lasso_coefficients_df$coefficient), decreasing = TRUE), ]
# Print sorted coefficients
print(sorted_lasso_coefficients)
#-------------#
# Assuming your data is in 'modified_df_new' and the response variable is 'num'
library(glmnet)
# Extracting predictors and response variable
response_variable <- "num"
predictors <- m_df[, names(m_df) != response_variable, drop = FALSE]
response <- m_df[, response_variable]
# Scale the predictors
scaled_predictors <- scale(predictors)
# Convert predictors to matrix
predictors_matrix <- as.matrix(scaled_predictors)
# Fit a Ridge (L2 regularization) model using cross-validation
ridge_model <- cv.glmnet(predictors_matrix, response, alpha = 0)
# Print the cross-validated results
print(ridge_model)
# Plot the cross-validated mean squared error as a function of log(lambda)
plot(ridge_model)
# Identify the optimal lambda (minimizing mean cross-validated error)
best_lambda <- ridge_model$lambda.min
cat("Best Lambda:", best_lambda, "\n")
# Get feature coefficients for the optimal lambda
ridge_coefficients <- coef(ridge_model, s = best_lambda)
# Print the coefficients (including potentially nonzero coefficients)
print(ridge_coefficients)
# Extract the selected features
selected_features <- rownames(ridge_coefficients)
cat("Selected Features:", selected_features, "\n")
# Assuming 'ridge_model' is the result of the Ridge regression model
# Get feature coefficients for the optimal lambda
ridge_coefficients <- coef(ridge_model, s = best_lambda)
# Extract the feature names and corresponding coefficients
feature_info <- data.frame(
Feature = rownames(ridge_coefficients),
Coefficient = as.numeric(ridge_coefficients)
)
# Sort features by absolute coefficient value
sorted_features <- feature_info[order(abs(feature_info$Coefficient), decreasing = TRUE), ]
# Print or inspect the sorted features
print(sorted_features)
#####------#####
# Assuming your dataframe is modified_df_new
model <- lm(num ~ ., data = m_df)
summary_stats <- summary(model)
# Extract t-statistics and p-values
t_statistics <- coef(summary_stats)[, "t value"]
p_values <- coef(summary_stats)[, "Pr(>|t|)"]
p_values <- round(p_values, 2)
print(p_values)
# Set significance level
significance_level <- 0.05
# Select features based on significance level
selected_features <- names(which(p_values < significance_level))
# Print selected features
cat("Selected Features:\n")
print(selected_features)
# Plotting p-values
barplot(p_values, names.arg = names(p_values), col = ifelse(p_values < significance_level, "blue", "gray"), main = "P-values for Each Feature", ylab = "P-value", col.main = "darkblue")
abline(h = significance_level, col = "red", lty = 2)
options(scipen = 999)
##########################
k_data_test <- m_df
# Assuming your dataframe is named 'modified_df_new' and you want to drop the 'num' column
k_data_test <- k_data_test[, -which(names(k_data_test) == "num")]
# Scale the data
scaled_data <- scale(k_data_test)
View(scaled_data)
k <- 5
kmeans_result <- kmeans(scaled_data, centers = k, nstart = 20)  # nstart for multiple initializations
# Assign cluster labels to the original dataset
k_data_test$cluster <- as.factor(kmeans_result$cluster)
# Print cluster centers
cat("Cluster Centers:\n")
print(kmeans_result$centers)
# Visualize the clustering results
plot(scaled_data, col = k_data_test$cluster)
# Assuming your dataframe is named 'scaled_data', and you have k-means results in 'kmeans_result'
library(factoextra)
# Example: Use PCA for dimensionality reduction
pca_data <- prcomp(scaled_data)
reduced_data <- pca_data$x[, 1:2]  # Choose the number of components
kmeans_result <- kmeans(reduced_data, centers = k)
# Visualize k-means clustering results
fviz_cluster(kmeans_result, data = scaled_data, geom = "point", stand = FALSE, frame.type = "norm")
#########
# Assuming 'data' is your data frame with the target variable named 'target'
# Adjust column names accordingly
# Example:
X <- subset(m_df, select = -num)
y <- m_df$num
# Perform one-way ANOVA for each feature
f_statistics <- sapply(X, function(column) {
result <- oneway.test(column ~ y)
return(result$statistic)
})
# Set your significance level (e.g., 0.05)
significance_level <- 0.3
# Identify significant features based on p-values
significant_features <- names(f_statistics[f_statistics > qf(1 - significance_level, 1, length(y) - 2)])
# Print significant features
cat("Significant Features:\n")
cat(significant_features,sep=",")
#####################################
# Assuming 'data' is your data frame with the target variable named 'target'
# Adjust column names accordingly
# Example:
X <- subset(m_df, select = -num)
y <- m_df$num
# Set a threshold for variance (adjust as needed)
variance_threshold <- 0.1
# Calculate variances for each feature
feature_variances <- apply(X, 2, var)
# Identify features with variance above the threshold
selected_features <- names(feature_variances[feature_variances > variance_threshold])
# Print selected features
cat("Selected Features:\n")
cat(selected_features, sep=", ")
###################################
#Since "lvx3" and "rldv5e" are least significant based on lasso and ridge, we are dropping them based on mannual anzlysis
column_names_todrop_rf <- c("rldv5e", "lvx3")
m_df <- m_df[, !colnames(m_df) %in% column_names_todrop_rf]
View(m_df)
##################################
library(dplyr)
library(ggplot2)
library(plotly)
# Ensure 'num' is treated as a factor if it's categorical
m_df$num <- as.factor(m_df$num)
# Plot graphic
Plot1 <- ggplot(m_df, aes(x = num, y = age, fill = num)) +
geom_boxplot() +
scale_fill_manual(values = rainbow(length(unique(m_df$num)))) +  # Add rainbow colors
theme_minimal() +
theme(legend.position = "none")
# Convert ggplot to interactive plotly plot
ggplotly(Plot1)
##############################################3
latest_df <- m_df
str(latest_df)
# Assuming 'num' is a character column in latest_df
latest_df$num <- as.numeric(as.character(latest_df$num))
# Check if 'num' is now numeric
is_numeric <- is.numeric(latest_df$num)
# Print the result
print(is_numeric)
# Assuming 'num' is a numeric or integer column in latest_df
latest_df$num <- ifelse(latest_df$num > 1, 1, latest_df$num)
# Check the unique values in 'num' after conversion
unique(latest_df$num)
View(latest_df)
# Logistic Regression as an example
model <- glm(num ~ ., data = latest_df, family = "binomial")
summary(model)
# Model Evaluation
predicted_probs <- predict(model, newdata = latest_df, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# Assess accuracy
confusion_matrix <- table(latest_df$num, predicted_classes)
print(confusion_matrix)
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
print(accuracy)
####################
aic <- AIC(model)
bic <- BIC(model)
cat("AIC:", aic, "\n")
cat("BIC:", bic, "\n")
# ROC-AUC
library(pROC)
roc_curve <- roc(latest_df$num, predict(model, type = "response"))
roc_auc <- auc(roc_curve)
cat("ROC-AUC:", roc_auc, "\n")
#########################
# Assuming 'data' is your dataset with 'CAD' as the binary outcome variable
# and 'feature1', 'feature2', ... as predictor variables
# Split the data into training and testing sets
set.seed(123)  # Set seed for reproducibility
train_indices <- sample(1:nrow(latest_df), 0.8 * nrow(latest_df))
train_data <- latest_df[train_indices, ]
test_data <- latest_df[-train_indices, ]
# Logistic Regression Model Training
model <- glm(num ~ feature1 + feature2 + ..., data = train_data, family = "binomial")
# Logistic Regression Model Training
model <- glm(num ~ ., data = train_data, family = "binomial")
# Model Evaluation on Training Set
train_predicted_probs <- predict(model, newdata = train_data, type = "response")
train_predicted_classes <- ifelse(train_predicted_probs > 0.5, 1, 0)
# Training Error
train_confusion_matrix <- table(train_data$num, train_predicted_classes)
train_accuracy <- sum(diag(train_confusion_matrix)) / sum(train_confusion_matrix)
cat("Training Accuracy:", train_accuracy, "\n")
# Model Evaluation on Testing Set
test_predicted_probs <- predict(model, newdata = test_data, type = "response")
test_predicted_classes <- ifelse(test_predicted_probs > 0.5, 1, 0)
# Testing Error
test_confusion_matrix <- table(test_data$num, test_predicted_classes)
test_accuracy <- sum(diag(test_confusion_matrix)) / sum(test_confusion_matrix)
cat("Testing Accuracy:", test_accuracy, "\n")
# Model Evaluation
predicted_probs <- predict(model, newdata = latest_df, type = "response")
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# Assess accuracy
confusion_matrix <- table(latest_df$num, predicted_classes)
print(confusion_matrix)
